{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.utils import np_utils\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.X_valid = None\n",
    "        self.X_test = None\n",
    "        self.Y_train = None\n",
    "        self.Y_valid = None\n",
    "        self.Y_test = None\n",
    "        self.images_list = []\n",
    "        self.labels_list = []\n",
    "        \n",
    "\n",
    "    # read and resize\n",
    "    def read_resize_image(self, full_name, resized_height = 50):\n",
    "        image = cv2.imread(full_name)\n",
    "        resized_image = imutils.resize(image, height = resized_height)\n",
    "        return resized_image    \n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    def get_image_array(self, file_path):\n",
    "\n",
    "        images, labels = self.traverse_image_dir(file_path)\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def read(self, img_rows=50, img_cols=50, img_channels=3, nb_classes=2):\n",
    "\n",
    "        images, labels = self.get_image_array(r'C:\\Users\\dbsnail\\ImageFolder\\data_large')\n",
    "        print(\"Images shape {}, label shape {}, ratio of authorized data {}\".format(images.shape, labels.shape, labels.mean()))\n",
    "    \n",
    "        # numpy.reshape\n",
    "        X_train_valid, X_test, y_train_valid, y_test = train_test_split(images, labels, test_size=0.25, random_state=123)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, train_size=0.7, random_state=200)\n",
    "        \n",
    "        del images\n",
    "        del labels\n",
    "        #reshape\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "        X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 3)\n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "\n",
    "        # the data, shuffled and split between train and test sets\n",
    "        print('X_train shape:', X_train.shape)\n",
    "        print(X_train.shape[0], 'train samples')\n",
    "        print(X_valid.shape[0], 'valid samples')\n",
    "        print(X_test.shape[0], 'test samples')\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "        Y_valid = np_utils.to_categorical(y_valid, nb_classes)\n",
    "        Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "        \n",
    "        # scale the input data to the range [0,1]\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_valid = X_valid.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_valid /= 255\n",
    "        X_test /= 255\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_valid = X_valid\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_valid = Y_valid\n",
    "        self.Y_test = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape (15074, 50, 50, 3), label shape (15074,), ratio of authorized data 0.48912033965768875\n",
      "X_train shape: (7913, 50, 50, 3)\n",
      "7913 train samples\n",
      "3392 valid samples\n",
      "3769 test samples\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of X_train: 0.422762\n",
      "Standard deviation of X_train: 0.263912\n",
      "Mean of X_valid: 0.425308\n",
      "Standard deviation  of X_valid: 0.265774\n",
      "Mean of X_test: 0.42328\n",
      "Standard deviation of X_test: 0.26611\n"
     ]
    }
   ],
   "source": [
    "X_train = dataset.X_train\n",
    "X_valid = dataset.X_valid\n",
    "X_test = dataset.X_test\n",
    "\n",
    "print('Mean of X_train:', np.mean(X_train))\n",
    "print('Standard deviation of X_train:', np.std(X_train))\n",
    "print('Mean of X_valid:', np.mean(X_valid))\n",
    "print('Standard deviation  of X_valid:', np.std(X_valid))\n",
    "print('Mean of X_test:', np.mean(X_test))\n",
    "print('Standard deviation of X_test:', np.std(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of X_train: 0.5\n",
      "Standard deviation of X_train: 0.5\n",
      "Mean of X_valid: 0.5\n",
      "Standard deviation  of X_valid: 0.5\n",
      "Mean of X_test: 0.5\n",
      "Standard deviation of X_test: 0.5\n"
     ]
    }
   ],
   "source": [
    "Y_train = dataset.Y_train\n",
    "Y_valid = dataset.Y_valid\n",
    "Y_test = dataset.Y_test\n",
    "\n",
    "print('Mean of X_train:', np.mean(Y_train))\n",
    "print('Standard deviation of X_train:', np.std(Y_train))\n",
    "print('Mean of X_valid:', np.mean(Y_valid))\n",
    "print('Standard deviation  of X_valid:', np.std(Y_valid))\n",
    "print('Mean of X_test:', np.mean(Y_test))\n",
    "print('Standard deviation of X_test:', np.std(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    FILE_PATH = r'C:\\Users\\dbsnail\\ImageProject\\model_with_aug.h5'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, dataset, nb_classes=2):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Convolution2D(32, 3, 3, border_mode='same', activation = 'relu', input_shape=dataset.X_train.shape[1:]))\n",
    "        self.model.add(Convolution2D(32, 3, 3, border_mode='same', activation = 'relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Convolution2D(48, 3, 3, border_mode='same', activation = 'relu'))\n",
    "        self.model.add(Convolution2D(48, 3, 3, border_mode='same', activation = 'relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #self.model.add(Dropout(0.25))\n",
    "        \n",
    "        self.model.add(Convolution2D(64, 3, 3, border_mode='same', activation = 'relu'))\n",
    "        self.model.add(Convolution2D(64, 3, 3, border_mode='same', activation = 'relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))        \n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(960))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(nb_classes))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        self.model.summary()\n",
    "     \n",
    "    def train(self, dataset, batch_size=30, nb_epoch=30, data_augmentation=True):\n",
    "        \n",
    "        # let's train the model using SGD + momentum (how original).\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self.model.compile(loss='categorical_crossentropy', ##'binary_crossentropy'\n",
    "                           optimizer=sgd,\n",
    "                           metrics=['accuracy'])  #['precision']) #\n",
    "        if not data_augmentation:\n",
    "            print('Not using data augmentation.')\n",
    "            self.model.fit(dataset.X_train, dataset.Y_train,\n",
    "                           batch_size=batch_size,\n",
    "                           nb_epoch=nb_epoch,\n",
    "                           validation_data=(dataset.X_valid, dataset.Y_valid),\n",
    "                           shuffle=True)\n",
    "        else:\n",
    "            print('Using real-time data augmentation.')\n",
    "\n",
    "            # This will do preprocessing and realtime data augmentation\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,             # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,              # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,   # divide each input by its std\n",
    "                zca_whitening=False,                  # apply ZCA whitening\n",
    "                rotation_range=0.,                    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.,                 # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.,                # randomly shift images vertically (fraction of total height)\n",
    "                channel_shift_range=0.2,\n",
    "                fill_mode = 'nearest',        # Points outside the boundaries of the input are filled according to the given mode.\n",
    "                horizontal_flip=False,                 # randomly flip images\n",
    "                vertical_flip=False)                  # randomly flip images\n",
    "\n",
    "            # compute quantities required for featurewise normalization\n",
    "            # (std, mean, and principal components if ZCA whitening is applied)\n",
    "            datagen.fit(dataset.X_train)\n",
    "\n",
    "            # fit the model on the batches generated by datagen.flow()\n",
    "            self.model.fit_generator(datagen.flow(dataset.X_train, dataset.Y_train,\n",
    "                                                  batch_size=batch_size),\n",
    "                                     samples_per_epoch=dataset.X_train.shape[0],\n",
    "                                     nb_epoch=nb_epoch,\n",
    "                                     validation_data=(dataset.X_valid, dataset.Y_valid))\n",
    "\n",
    "    def save(self, file_path=FILE_PATH):\n",
    "        print('Model Saved.')\n",
    "        self.model.save(file_path)\n",
    "\n",
    "    def load(self, file_path=FILE_PATH):\n",
    "        print('Model Loaded.')\n",
    "        self.model = load_model(file_path)\n",
    "\n",
    "    def predict(self, image):\n",
    "        image = image.reshape((1, 50, 50, 3))\n",
    "        image = image.astype('float32')\n",
    "        image /= 255\n",
    "        result = self.model.predict_proba(image)\n",
    "        print(result)\n",
    "        result = self.model.predict_classes(image)\n",
    "\n",
    "        return result[0]\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        score = self.model.evaluate(dataset.X_test, dataset.Y_test, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_67 (Convolution2D) (None, 50, 50, 32)    896         convolution2d_input_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_68 (Convolution2D) (None, 50, 50, 32)    9248        convolution2d_67[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_34 (MaxPooling2D)   (None, 25, 25, 32)    0           convolution2d_68[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_69 (Convolution2D) (None, 25, 25, 48)    13872       maxpooling2d_34[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_70 (Convolution2D) (None, 25, 25, 48)    20784       convolution2d_69[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_35 (MaxPooling2D)   (None, 12, 12, 48)    0           convolution2d_70[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_71 (Convolution2D) (None, 12, 12, 64)    27712       maxpooling2d_35[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_72 (Convolution2D) (None, 12, 12, 64)    36928       convolution2d_71[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_36 (MaxPooling2D)   (None, 6, 6, 64)      0           convolution2d_72[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 6, 6, 64)      0           maxpooling2d_36[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 2304)          0           dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 960)           2212800     flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 960)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (None, 960)           0           activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 2)             1922        dropout_28[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 2)             0           dense_24[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2,324,162\n",
      "Trainable params: 2,324,162\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/30\n",
      "7913/7913 [==============================] - 21s - loss: 0.2783 - acc: 0.8859 - val_loss: 0.0639 - val_acc: 0.9782\n",
      "Epoch 2/30\n",
      "7913/7913 [==============================] - 20s - loss: 0.0653 - acc: 0.9765 - val_loss: 0.0197 - val_acc: 0.9941\n",
      "Epoch 3/30\n",
      "7913/7913 [==============================] - 20s - loss: 0.0336 - acc: 0.9888 - val_loss: 0.0108 - val_acc: 0.9976\n",
      "Epoch 4/30\n",
      "7913/7913 [==============================] - 20s - loss: 0.0214 - acc: 0.9930 - val_loss: 0.0097 - val_acc: 0.9982\n",
      "Epoch 5/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0060 - val_acc: 0.9982\n",
      "Epoch 6/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0113 - acc: 0.9955 - val_loss: 0.0054 - val_acc: 0.9982\n",
      "Epoch 7/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0031 - val_acc: 0.9985\n",
      "Epoch 8/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0061 - acc: 0.9976 - val_loss: 0.0119 - val_acc: 0.9950\n",
      "Epoch 9/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0066 - acc: 0.9971 - val_loss: 0.0150 - val_acc: 0.9962\n",
      "Epoch 10/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0094 - acc: 0.9973 - val_loss: 0.0038 - val_acc: 0.9988\n",
      "Epoch 11/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0018 - val_acc: 0.9994\n",
      "Epoch 12/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 13/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0031 - acc: 0.9986 - val_loss: 0.0063 - val_acc: 0.9979\n",
      "Epoch 14/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0047 - val_acc: 0.9982\n",
      "Epoch 15/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0051 - val_acc: 0.9988\n",
      "Epoch 16/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0062 - acc: 0.9977 - val_loss: 0.0113 - val_acc: 0.9962\n",
      "Epoch 17/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0017 - val_acc: 0.9997\n",
      "Epoch 18/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0012 - acc: 0.9997 - val_loss: 9.2717e-04 - val_acc: 0.9994\n",
      "Epoch 19/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0022 - acc: 0.9990 - val_loss: 0.0033 - val_acc: 0.9985\n",
      "Epoch 20/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0027 - acc: 0.9986 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Epoch 21/30\n",
      "7913/7913 [==============================] - 19s - loss: 8.1322e-04 - acc: 0.9996 - val_loss: 9.8705e-04 - val_acc: 0.9997\n",
      "Epoch 22/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0016 - acc: 0.9996 - val_loss: 9.5600e-04 - val_acc: 0.9994\n",
      "Epoch 23/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "Epoch 24/30\n",
      "7913/7913 [==============================] - 19s - loss: 2.1608e-04 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9988\n",
      "Epoch 25/30\n",
      "7913/7913 [==============================] - 19s - loss: 2.2157e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9994\n",
      "Epoch 26/30\n",
      "7913/7913 [==============================] - 19s - loss: 3.4591e-04 - acc: 0.9997 - val_loss: 0.0041 - val_acc: 0.9991\n",
      "Epoch 27/30\n",
      "7913/7913 [==============================] - 19s - loss: 7.4079e-04 - acc: 0.9996 - val_loss: 2.4475e-04 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "7913/7913 [==============================] - 19s - loss: 0.0011 - acc: 0.9997 - val_loss: 9.1918e-04 - val_acc: 0.9994\n",
      "Epoch 29/30\n",
      "7913/7913 [==============================] - 19s - loss: 4.3394e-04 - acc: 0.9999 - val_loss: 9.4750e-04 - val_acc: 0.9994\n",
      "Epoch 30/30\n",
      "7913/7913 [==============================] - 19s - loss: 3.0331e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Model Saved.\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.build_model(dataset)\n",
    "model.train(dataset)\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n",
      "acc: 99.89%\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load()\n",
    "model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.29504853e-16   1.00000000e+00]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\sample1.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  8.80496591e-05   9.99911904e-01]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\sample2.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  5.65269693e-06   9.99994397e-01]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\sample3.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  6.67391445e-19   1.00000000e+00]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\sample4.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[ 0.97923434  0.02076572]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\chendaoming.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.28662248e-10   1.00000000e+00]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\zhourunfa.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.00000000e+00   1.08988068e-13]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\chenbaoguo.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[ 0.52811664  0.47188342]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\zhangguoli.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.00000000e+00   4.99704682e-14]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\linyongjian.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.00000000e+00   1.20920238e-14]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\lixuejian.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  1.00000000e+00   6.45176102e-15]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\wangziwen.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[ 0.72366351  0.27633649]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\tangguoqiang.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[  2.22215430e-15   1.00000000e+00]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(r'C:\\Users\\dbsnail\\ImageFolder\\test\\zhangyimou.jpg')\n",
    "resized_image = imutils.resize(image, height = 50)\n",
    "model.predict(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
